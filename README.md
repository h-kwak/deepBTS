# deepBTS
Deep Bidirectional Transcription Scan

deepBTS manual

```
Usage:
        deepBTS [OPTIONS] -o <Output file>

Option  Description
-p      Plus strand bedgraph
-m      Minus strand bedgraph
-n      ANN network configuration
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
make    Make training data from bedgraph files to the output file
-p      Plus strand bedgraph
-m      Minus strand bedgraph
-b      TRE bed file
--bin   Bin sizes (default = 50, 500 bp)
--rng   Scanning ranges (default = 500, 5000 bp)
--step  Step size (default = 50 bp, must be a divisor of bins and ranges)
--sr    Sample ratio (negative:positive, default = 1.5)
--p     Parameter file output (default = "ANN_parameters.txt")
- - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
train   Train and save ANN configuration to the output file
-t      Input training set files
--p     Parameter file input (default = "ANN_parameters.txt")
--con   Previous network configuration to continue training (default = none)
```

## Installation
### Requirement
Fast Artificial Neural Network library [FANN](https://github.com/libfann/fann)
### Make
```
make
```
## deepBTS
### Input data files
- Test PRO-seq data (both plus and minus strand bedgraph files)
- Trained network file
### Output
Bedgraph file of a deepBTS score specified by -o option
### Usage example
```
deepBTS -p proseq.pl.bg -m proseq.mn.bg -n output.net -o output.bg
```
## deepBTS make: makes a training data
### Requirements
Reference PRO-seq data (both plus and minus strand bedgraph files)
TRE bed file as a standard annotation.
### Output
Train data file specified by -o option
Automatically generates parameter file for neural network options (specified by --p option)
### Usage example
```
dpPRO make -p proseq.pl.bg -m proseq.mn.bg -b tre.bed -o output.train
```
### Advanced options
1. dpPRO make scans through the PRO-seq files and collects readcounts in a range of regions. Similar to microscopes that have more than one lens and offer multiple resolutions, dpPRO scans different ranges at multiple resolutions. By default, it scans +/- 500 bp region and +/- 5 kb regions in 50 bp and 500 bp resolution (bin sizes) respectively. dpPRO may improve performance by scanning extended regions at different resolutions. Note that if you are adding a smaller bin size, the step size option needs to decrease accordingly.
For example, add the following options to the command line:
--bin 25 50 500 --rng 250 1500 10000 --step 25
2. The command automatically generates a “ANN_parameters.txt” file that contains information about the neural network. It will replace the current ANN_parameters.txt file. You can save the parameter file in a different filename using the --p option.

## dpPRO train: trains a network from the training data

### Requirements
Train file generated from dpPRO make
Neural network parameter file (default is the ANN_parameter.txt, can select a different file using --p option)

### Output
Trained network file specified by -o option
Temporary network file specified by --tnet option, saved during the training process.
Prints information of the progression of the training to the console.

### Usage example

	dpPRO train -t output.train -o output.net

### Advanced options
1. You can use more than one training file. Simply list all the training files after the -t option. For example
	-t output1.train output2.train output3.train
2. The contents of the ANN parameter file can be edited to use different neural network designs. Use the ANN parameter file generated by the dpPRO make command, and change the number of layers, number of hidden, desired error, max iterations, and iterations between reports. Do not change the number of input neurons and number of output neurons from what is generated by the dpPRO make command. Number of input neurons is determined by the bins and the ranges of the dpPRO make command. Output neurons are either true or false states of the TSS annotation, and the number is fixed at 2. Number of layers include input, hidden, and output layers. By default, the number of layers is 3 (1 hidden) representing a shallow neural network. 4 or more num_layers means there are 2 or more hidden layers for a deep neural network. For deep neural networks, add lines to indicate the numbers of hidden neurons for each hidden layer. dpPRO currently support up to 3 hidden layers (num_layers max at 5 total). The numbers of hidden neurons in each layer are configurable. A starting point for the number of hidden neurons can be a geometric progression from the number of input neurons through the number of hidden neurons and to the number of output neurons. In case of a 1 hidden layer, num_hidden = sqrt( num_input * num_output ). 

#### Example of an ANN parameter file with 1 hidden layer
``` 
learning_rate=	0.1
num_layers=	3
num_input=	80
num_hidden=	12
num_output=	2
desired_error=	0.001
max_iterations=	2000
iterations_between_reports=	5
```
#### Example of an ANN parameter file with 2 hidden layers
```
learning_rate=	0.1
num_layers=	4
num_input=	80
num_hidden1=		24
num_hidden2=		6
num_output=	2
desired_error=	0.001
max_iterations=	1500
iterations_between_reports=	5
```

3. The output console can be piped to a file using the UNIX tee command. A simple R script can monitor the training progression in real time by plotting the epochs and errors. 
5. It is possible to continue training of a previously trained network file using the --con option. By default, this is set to null and dpPRO will start a new network unless the --con network file is specified. For new training, dpPRO will use a RPROP algorithm, but for continued training, dpPRO will use standard back propagation algorithm. 

